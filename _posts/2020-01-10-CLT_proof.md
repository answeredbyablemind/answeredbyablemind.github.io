---
title: 중심극한정리 증명
sidebar:
  nav: docs-ko
aside:
  toc: true
key: 20200110
tags: 확률통계
---
# 중심극한 정리의 증명에 필수적인 배경지식

## 확률 변수의 합과 확률 밀도함수의 convolution

독립적인 random variables X와 Y를 생각해보자. 이 때, X와 Y의 확률질량함수를 $m_1(x), m_2(x)$라고 하자. 

이 때, $Z=X+Y$로 정의되는 새로운 random variable 를 생각해보자. 임의의 정수 $z$에 대해서 random variable $Z$의 realization을 $z$라고 하고, 임의의 정수 $k$에 대해서 $X=k$일 때, $Z=X+Y$라는 관계식이 성립하기 위해서는 $Y=z-k$일 수 밖에 없다. 사건의 관점에서 보면 

$$(X=k)\text{ and }(Y=z-k)$$

가 성립해야하며, 이제 모든 정수 $k$에 대해 $Z=z$인 확률은 다음과 같이 결정할 수 있다.

$$P(Z=z) = \sum_{k=-\infty}^{\infty}{P(X=k)P(Y=z-k)}$$

따라서, 독립적인 random variables $X$와 $Y$의 합인 $Z$의 확률질량함수는 다음과 같이 나타낼 수 있다. 

$$m_3(j) = \sum_km_1(k)m_2(j-k)\notag$$

$$\text{for }j = \cdots, -2, -1, 0, 1, 2, \cdots$$

위와 같은 논리를 이용하여, n개의 독립적인 random variables $X_1, X_2, \cdots, X_n$에 대하여 n개의 random variables의 합을 $S=X_1+X_2+\cdots+X_n$로 나타낼 수 있으며, 이것은 $S_n=S_{n-1}+X_n$이라고 표현할 수 있다. 따라서 무수한 independent random variables의 합에 대한 확률질량함수(pdf)를 convolution을 이용해 구할 수 있다.
    
더 나아가 연속적인 random variables $X$와 $Y$에 대해서 다음이 성립한다. $X$와 $Y$의 확률밀도함수를 각각 $f(x)$와 $g(x)$라고 하자. 이 때, $f(x)$와 $g(x)$는 모든 실수에 대해서 정의되어 있다. 

이 때, $X+Y$의 확률밀도함수는 두 확률밀도함수의 convolution으로 표현할 수 있다.

$$(f*g)(z) = \int_{-\infty}^{\infty}f(z-y)g(y)dy = \int_{-\infty}^{\infty}g(z-x)f(x)dx$$

## characteristic function

 신호처리에서 푸리에 변환은 시간 함수를 시간 도메인에서 주파수 도메인으로 변환시켜주는 역할을 하고, 역 푸리에 변환은 주파수 도메인의 함수를 시간 도메인으로 변환시켜주는 역할을 한다. 또, 시간 영역의 함수와 그 쌍인 주파수 영역의 함수는 1:1 매핑을 이룬다.

 통계학에서는 이러한 관계를 이용하여 characteristic function이라는 개념을 만들었는데, 이것은 수학적으로 역 푸리에 변환과 동일한 식을 가지며, 푸리에 변환이 가지고 있는 성질들을 그대로 이어받으며, 통계학에서만 활용되는 pdf의 moment와 관련된 개념과 연관시켜 사용되기도 한다.

 확률통계학에서 임의의 random variable 에 대한 characteristic function은 다음과 같이 정의한다. 

| DEFINITION. characteristic function |
| --------- |
| 임의의 확률 밀도 함수 $f_X(x)$에 대한 characteristic function은 <br> <center> $$\phi_X(t) = E\left[e^{jtx}\right]=\int_{-\infty}^{\infty}e^{jtx}f_X(x)dx$$</center>|

characteristic function의 성질 중 CLT의 증명에 필요한 것을 꼽자면 다음과 같다.

① 각각의 Random Variable들은 고유(unique)의 characteristic function을 가진다. 즉, Random Variable 하나와 그에 상응하는 characteristic function은 1:1 mapping 관계를 가진다.

② 서로 독립인 $p$개의 Random Variables $X_1, X_2, \cdots, X_p$에 대해 다음이 성립한다. 위에서 독립 Random variables의 합의 pdf는 convolution으로 나타난다고 했다. 따라서 characteristic function의 domain에서 각각의 독립적인 random variable들의 합의 characteristic function은 각각의 독립적인 random variables의 characteristic function의 곱으로 나타낼 수 있다.

한편 맥클로린 급수를 이용하면 $e^{jtx}$를 다음과 같이 풀어서 생각할 수 있다.

$$e^{jtx}=\sum_{k=1}^{\infty}\frac{(jtx)^k}{k!}=1+jtx+\frac{(jtx)^2}{2!}+\frac{(jtx)^3}{3!}+\cdots = 1+jtx+\frac{(jtx)^2}{2!}+O(t^2)$$

이 때 $O(t^2)$는 네 번째 이후의 항을 합쳐서 생각한 것이다. 그러면 characteristic function을 계산하는 것은 pdf의 moment를 계산하는 것과 연관되게 된다. 다음을 보도록 하자. 임의의 확률 밀도 함수 $f_Y(y)$에 대하여,

$$\phi_Y(t) = \int_{-\infty}^{\infty}e^{jty}f_Y(t)dy=\int_{-\infty}^{\infty}\left\{1+jty-\frac{t^2}{2}y^2+O(t^2)\right\}f_Y(y)dy\notag$$

$$=\int_{-\infty}^{\infty}f_Y(y)dy + \int_{-\infty}^{\infty}yf_Y(y)dy - \frac{t^2}{2}\int_{-\infty}^{\infty}y^2f_Y(y)dy+O(t^2)\notag$$

$$=1+jtE\left[y\right]-\frac{t^2}{2}E\left[y^2\right]+O(t^2)$$

와 같다. 


# Central Limit Theorem의 증명

모집단으로부터 추출된 임의의 $N$개의 sample을 $X_i \sim iid(\mu, \sigma^2)$라고 하자. 그러면 sample mean은 

$$\bar{X}_N=\frac{1}{N}\sum_{i=1}^{N}X_i$$

이다. 그러면 $\bar{X}_N$의 기댓값은 다음과 같다.

$$E\left[\bar{X}_N\right] = E\left[\frac{1}{N}\sum_{i=1}^{N}X_i\right]

=\frac{1}{N}\sum_{i=1}^{N}E\left[X_i\right] = \frac{1}{N}(N\times\mu) = \mu$$

또, $\bar{X}_N$의 분산은 다음과 같다.

$$Var\left[\bar{X}_N\right] = Var\left[\frac{1}{N}\sum_{i=1}^{N}X_i\right]=\frac{1}{N^2}\sum_{i=1}^{N}Var\left[X_i\right]=\frac{1}{N^2}\left(N\sigma^2\right)=\frac{\sigma^2}{N}$$

이 때, $\bar{X}_N$을 정규화 시킨 변수를 $Z_N$이라고 하면,

$$Z_N = \frac{\bar{X}_N-\mu}{\left(\frac{\sigma}{\sqrt{N}}\right)}

=\frac{N\bar{X}_N-N\mu}{\sqrt{N}\mu} = \frac{\sum_{i=1}^{N}(X_i-\mu)}{\sqrt{N}\sigma} \notag$$

$$=\sum_{i=1}^{N}\left(\frac{X_i-\mu}{\sqrt{N}\sigma}\right)

= \sum_{i=1}^{N}\frac{y_i}{\sqrt{N}}$$

여기서 $y_i = (X_i-\mu)/\sigma$이므로 $E[y_i]=0$이고 $Var[y_i]=Var[X_i]/\sigma^2=1$이다.

그러면 $\phi_Y(y)$는 다음과 같이 계산할 수 있게 된다.

$$\phi_Y(y) = 1+0-\frac{t^2}{2}+O(t^2) = 1-\frac{t^2}{2}+O(t^2)$$

다시 여기서 

$$Z_N = \sum_{i=1}^{N}\frac{y_i}{\sqrt{N}}$$

인데, 랜덤 변수의 합은 확률 밀도 함수에서는 convolution으로 표현된다는 사실을 앞에서 확인했다. characteristic function을 자세히 보면 이것은 역 푸리에 변환과 같은 모양을 가지고 있다는 것 또한 확인할 수 있는데, 신호 처리 이론에서 주파수 대역에서의 함수의 convolution은 시간 대역에서는 해당 함수의 시간 변환의 곱으로 표현된다는 것을 알고 있다. 

따라서 $Z_N$은 $Y$들의 합으로 구성되어 있고, 이것은 $f_Y(y)$의 컨볼루션으로 나타날 것이며, 다시 한 번 characteristic function의 domain에서는 characteristic function들의 곱으로 표현할 수 있다. 따라서 다음이 성립한다.

$$\phi_{Z_N}(t) = [\phi_Y(t/\sqrt(N))]^N = [1-\frac{t^2}{2N}+O(\frac{t^2}{N})]^N$$

이 때, $N$이 무한히 커지는 경우를 상정하자.

$$\lim_{N\rightarrow\infty}\phi_{Z_N}(t) = \lim_{N\rightarrow\infty}[1-\frac{t^2}{2N}+O(\frac{t^2}{N})]^N$$

그러면 $O(\frac{t^2}{N})$은 $\frac{t^2}{2N}$보다 더 빨리 0으로 수렴한다는 사실을 알 수 있다. 따라서, 위 극한은 다음으로 수렴하게 된다.

$$\lim_{N\rightarrow \infty}\phi_{Z_N}(t) = \lim_{N\rightarrow\infty}[1-\frac{t^2}{2N}]^N=e^{-t^2/2}$$

한편, 평균이 0이고 표준편차가 1인 가우시안 분포의 characteristic function이 $e^{-t^2/2}$인데, characteristic function은 하나의 Random Variable과 1:1 매핑 관계를 가지고 있으므로 표본 평균의 분포는 가우시안 분포를 가지게 된다.